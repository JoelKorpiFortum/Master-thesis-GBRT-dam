{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb60609e-9b1f-4738-aacf-42bcdf8f28ea",
   "metadata": {},
   "source": [
    "## Notebook execution in Jupyter for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b04d890-fbb2-47ef-82a5-272960b9bd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/Master-thesis-GBRT-dam'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cdcc61-9e01-4102-9e94-fc1e2c57aa42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "TARGET: GV1\n",
      "\u001b[32m[I 2025-03-25 22:25:16,184]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:00,  3.02it/s]\n",
      "1th fold: LGBMRegressor RMSE: 61.9984\n",
      "2th fold: LGBMRegressor RMSE: 31.0557\n",
      "3th fold: LGBMRegressor RMSE: 3.2389\n",
      "\n",
      "LGBMRegressor average RMSE: 32.0977\n",
      "\u001b[32m[I 2025-03-25 22:25:17,181]\u001b[0m Trial 0 finished with value: 32.09765809194851 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 32.09765809194851.\u001b[0m\n",
      "3it [00:00,  4.15it/s]\n",
      "1th fold: LGBMRegressor RMSE: 31.8711\n",
      "2th fold: LGBMRegressor RMSE: 37.9226\n",
      "3th fold: LGBMRegressor RMSE: 3.5038\n",
      "\n",
      "LGBMRegressor average RMSE: 24.4325\n",
      "\u001b[32m[I 2025-03-25 22:25:17,907]\u001b[0m Trial 1 finished with value: 24.432499757037714 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 1 with value: 24.432499757037714.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV1\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.08648901138843648\n",
      "RMSE_test: 3.9560092991536853\n",
      "MAE_test: 2.32978210778321\n",
      "Willmott's d Test: 0.7937429037551396\n",
      "Nash-Sutcliffe Test: -0.4783429485642634\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 2.7549 seconds\n",
      "\n",
      "TARGET: GV3\n",
      "\u001b[32m[I 2025-03-25 22:25:18,889]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:00,  3.08it/s]\n",
      "1th fold: LGBMRegressor RMSE: 5.8662\n",
      "2th fold: LGBMRegressor RMSE: 27.0676\n",
      "3th fold: LGBMRegressor RMSE: 4.4642\n",
      "\n",
      "LGBMRegressor average RMSE: 12.4660\n",
      "\u001b[32m[I 2025-03-25 22:25:19,864]\u001b[0m Trial 0 finished with value: 12.466018541986486 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 12.466018541986486.\u001b[0m\n",
      "3it [00:00,  3.70it/s]\n",
      "1th fold: LGBMRegressor RMSE: 12.5801\n",
      "2th fold: LGBMRegressor RMSE: 144.6816\n",
      "3th fold: LGBMRegressor RMSE: 5.8837\n",
      "\n",
      "LGBMRegressor average RMSE: 54.3818\n",
      "\u001b[32m[I 2025-03-25 22:25:20,678]\u001b[0m Trial 1 finished with value: 54.381818471562106 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 12.466018541986486.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV3\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.08008730414955731\n",
      "RMSE_test: 7.545638177331732\n",
      "MAE_test: 3.8091010153296656\n",
      "Willmott's d Test: 0.6087667121323026\n",
      "Nash-Sutcliffe Test: -2.439246850128077\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 5.6280 seconds\n",
      "\n",
      "TARGET: GV51\n",
      "\u001b[32m[I 2025-03-25 22:25:21,758]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:00,  3.02it/s]\n",
      "1th fold: LGBMRegressor RMSE: 16.3293\n",
      "2th fold: LGBMRegressor RMSE: 29.8227\n",
      "3th fold: LGBMRegressor RMSE: 48.1472\n",
      "\n",
      "LGBMRegressor average RMSE: 31.4331\n",
      "\u001b[32m[I 2025-03-25 22:25:22,753]\u001b[0m Trial 0 finished with value: 31.433068208041046 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 31.433068208041046.\u001b[0m\n",
      "3it [00:00,  3.63it/s]\n",
      "1th fold: LGBMRegressor RMSE: 128.0073\n",
      "2th fold: LGBMRegressor RMSE: 31.9165\n",
      "3th fold: LGBMRegressor RMSE: 12.7499\n",
      "\n",
      "LGBMRegressor average RMSE: 57.5579\n",
      "\u001b[32m[I 2025-03-25 22:25:23,583]\u001b[0m Trial 1 finished with value: 57.55789893744199 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 31.433068208041046.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV51\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.0975463983211157\n",
      "RMSE_test: 3.021251253302488\n",
      "MAE_test: 1.341245061218314\n",
      "Willmott's d Test: 0.9029165775873396\n",
      "Nash-Sutcliffe Test: 0.48977932874003427\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 8.5516 seconds\n",
      "\n",
      "TARGET: MB4\n",
      "\u001b[32m[I 2025-03-25 22:25:24,686]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:00,  3.06it/s]\n",
      "1th fold: LGBMRegressor RMSE: 4.6463\n",
      "2th fold: LGBMRegressor RMSE: 413.1048\n",
      "3th fold: LGBMRegressor RMSE: 916.8139\n",
      "\n",
      "LGBMRegressor average RMSE: 444.8550\n",
      "\u001b[32m[I 2025-03-25 22:25:25,670]\u001b[0m Trial 0 finished with value: 444.8550114236578 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 444.8550114236578.\u001b[0m\n",
      "3it [00:00,  3.62it/s]\n",
      "1th fold: LGBMRegressor RMSE: 3.1738\n",
      "2th fold: LGBMRegressor RMSE: 233.1178\n",
      "3th fold: LGBMRegressor RMSE: 1912.9975\n",
      "\n",
      "LGBMRegressor average RMSE: 716.4297\n",
      "\u001b[32m[I 2025-03-25 22:25:26,502]\u001b[0m Trial 1 finished with value: 716.4297078464991 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 444.8550114236578.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB4\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.27899239759969213\n",
      "RMSE_test: 128.0552895769427\n",
      "MAE_test: 52.881161761353475\n",
      "Willmott's d Test: 0.03091773851164592\n",
      "Nash-Sutcliffe Test: -829.8129857294487\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 11.4755 seconds\n",
      "\n",
      "TARGET: MB8\n",
      "\u001b[32m[I 2025-03-25 22:25:27,612]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  2.83it/s]\n",
      "1th fold: LGBMRegressor RMSE: 253.6638\n",
      "2th fold: LGBMRegressor RMSE: 444.7392\n",
      "3th fold: LGBMRegressor RMSE: 2865.6043\n",
      "\n",
      "LGBMRegressor average RMSE: 1188.0024\n",
      "\u001b[32m[I 2025-03-25 22:25:28,674]\u001b[0m Trial 0 finished with value: 1188.0024295889582 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 1188.0024295889582.\u001b[0m\n",
      "3it [00:00,  3.08it/s]\n",
      "1th fold: LGBMRegressor RMSE: 2075.7886\n",
      "2th fold: LGBMRegressor RMSE: 79.3281\n",
      "3th fold: LGBMRegressor RMSE: 365.9835\n",
      "\n",
      "LGBMRegressor average RMSE: 840.3667\n",
      "\u001b[32m[I 2025-03-25 22:25:29,650]\u001b[0m Trial 1 finished with value: 840.3667402969181 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 1 with value: 840.3667402969181.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB8\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.7765824815506543\n",
      "RMSE_test: 4060.2599723678295\n",
      "MAE_test: 1416.3304597741912\n",
      "Willmott's d Test: 5.416187393603167e-05\n",
      "Nash-Sutcliffe Test: -10599212.819530357\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 14.7082 seconds\n",
      "\n",
      "TARGET: MB10\n",
      "\u001b[32m[I 2025-03-25 22:25:30,902]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  2.72it/s]\n",
      "1th fold: LGBMRegressor RMSE: 8168.4524\n",
      "2th fold: LGBMRegressor RMSE: 3575.4952\n",
      "3th fold: LGBMRegressor RMSE: 98.3806\n",
      "\n",
      "LGBMRegressor average RMSE: 3947.4427\n",
      "\u001b[32m[I 2025-03-25 22:25:32,008]\u001b[0m Trial 0 finished with value: 3947.4427397760956 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 3947.4427397760956.\u001b[0m\n",
      "3it [00:01,  1.80it/s]\n",
      "1th fold: LGBMRegressor RMSE: 7446291525980.5156\n",
      "2th fold: LGBMRegressor RMSE: 884.3003\n",
      "3th fold: LGBMRegressor RMSE: 186.8118\n",
      "\n",
      "LGBMRegressor average RMSE: 2482097175683.8755\n",
      "\u001b[32m[I 2025-03-25 22:25:33,676]\u001b[0m Trial 1 finished with value: 2482097175683.8755 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 3947.4427397760956.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB10\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.3330265031434321\n",
      "RMSE_test: 455.5087308208678\n",
      "MAE_test: 279.121219769196\n",
      "Willmott's d Test: 0.0027019874794825816\n",
      "Nash-Sutcliffe Test: -122576.93950302861\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 18.7179 seconds\n",
      "\n",
      "TARGET: MB18\n",
      "\u001b[32m[I 2025-03-25 22:25:34,856]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  2.93it/s]\n",
      "1th fold: LGBMRegressor RMSE: 207.1185\n",
      "2th fold: LGBMRegressor RMSE: 659.8927\n",
      "3th fold: LGBMRegressor RMSE: 457.2923\n",
      "\n",
      "LGBMRegressor average RMSE: 441.4345\n",
      "\u001b[32m[I 2025-03-25 22:25:35,883]\u001b[0m Trial 0 finished with value: 441.4344905726994 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 441.4344905726994.\u001b[0m\n",
      "3it [00:00,  3.26it/s]\n",
      "1th fold: LGBMRegressor RMSE: 66.2994\n",
      "2th fold: LGBMRegressor RMSE: 144.8678\n",
      "3th fold: LGBMRegressor RMSE: 446.2241\n",
      "\n",
      "LGBMRegressor average RMSE: 219.1304\n",
      "\u001b[32m[I 2025-03-25 22:25:36,807]\u001b[0m Trial 1 finished with value: 219.1304297945861 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 1 with value: 219.1304297945861.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB18\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.7073752813128316\n",
      "RMSE_test: 1511.1807250663974\n",
      "MAE_test: 882.8959465489263\n",
      "Willmott's d Test: 0.0009060793240114329\n",
      "Nash-Sutcliffe Test: -141939.4747353963\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 21.8315 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./models/LightGBM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c953144f-b001-4106-b9eb-e293f4096a3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: GV1\n",
      "\u001b[32m[I 2025-03-25 22:25:39,242]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  1.79it/s]\n",
      "1th fold: XGBRegressor RMSE: 0.6523\n",
      "2th fold: XGBRegressor RMSE: 0.2346\n",
      "3th fold: XGBRegressor RMSE: 1.0487\n",
      "\n",
      "XGBRegressor average RMSE: 0.6452\n",
      "\u001b[32m[I 2025-03-25 22:25:40,925]\u001b[0m Trial 0 finished with value: 0.6452006403183347 and parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}. Best is trial 0 with value: 0.6452006403183347.\u001b[0m\n",
      "3it [00:05,  1.94s/it]\n",
      "1th fold: XGBRegressor RMSE: 0.4838\n",
      "2th fold: XGBRegressor RMSE: 0.2492\n",
      "3th fold: XGBRegressor RMSE: 0.9264\n",
      "\n",
      "XGBRegressor average RMSE: 0.5531\n",
      "\u001b[32m[I 2025-03-25 22:25:46,752]\u001b[0m Trial 1 finished with value: 0.5531211829421827 and parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}. Best is trial 1 with value: 0.5531211829421827.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV1\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.08223438238361788\n",
      "RMSE_test: 0.273698574591439\n",
      "MAE_test: 0.20410508050845697\n",
      "Willmott's d Test: 0.9982544208838527\n",
      "Nash-Sutcliffe Test: 0.9929237094300658\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 10.9077 seconds\n",
      "\n",
      "TARGET: GV3\n",
      "\u001b[32m[I 2025-03-25 22:25:50,126]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  1.78it/s]\n",
      "1th fold: XGBRegressor RMSE: 0.9445\n",
      "2th fold: XGBRegressor RMSE: 0.5514\n",
      "3th fold: XGBRegressor RMSE: 1.4462\n",
      "\n",
      "XGBRegressor average RMSE: 0.9807\n",
      "\u001b[32m[I 2025-03-25 22:25:51,818]\u001b[0m Trial 0 finished with value: 0.9806951467261423 and parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}. Best is trial 0 with value: 0.9806951467261423.\u001b[0m\n",
      "3it [00:06,  2.21s/it]\n",
      "1th fold: XGBRegressor RMSE: 0.9342\n",
      "2th fold: XGBRegressor RMSE: 0.3290\n",
      "3th fold: XGBRegressor RMSE: 1.3928\n",
      "\n",
      "XGBRegressor average RMSE: 0.8853\n",
      "\u001b[32m[I 2025-03-25 22:25:58,448]\u001b[0m Trial 1 finished with value: 0.8853260696224591 and parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}. Best is trial 1 with value: 0.8853260696224591.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV3\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.10264418785007226\n",
      "RMSE_test: 0.6648168469832604\n",
      "MAE_test: 0.4335836860799953\n",
      "Willmott's d Test: 0.9930135735012835\n",
      "Nash-Sutcliffe Test: 0.9733022029128114\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 22.9370 seconds\n",
      "\n",
      "TARGET: GV51\n",
      "\u001b[32m[I 2025-03-25 22:26:02,196]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  1.80it/s]\n",
      "1th fold: XGBRegressor RMSE: 0.8347\n",
      "2th fold: XGBRegressor RMSE: 0.5644\n",
      "3th fold: XGBRegressor RMSE: 1.2055\n",
      "\n",
      "XGBRegressor average RMSE: 0.8682\n",
      "\u001b[32m[I 2025-03-25 22:26:03,870]\u001b[0m Trial 0 finished with value: 0.8682049666222215 and parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}. Best is trial 0 with value: 0.8682049666222215.\u001b[0m\n",
      "3it [00:06,  2.25s/it]\n",
      "1th fold: XGBRegressor RMSE: 0.9461\n",
      "2th fold: XGBRegressor RMSE: 0.4954\n",
      "3th fold: XGBRegressor RMSE: 1.0709\n",
      "\n",
      "XGBRegressor average RMSE: 0.8375\n",
      "\u001b[32m[I 2025-03-25 22:26:10,636]\u001b[0m Trial 1 finished with value: 0.8374627862634313 and parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}. Best is trial 1 with value: 0.8374627862634313.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV51\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.11983786620760582\n",
      "RMSE_test: 0.4237773122761143\n",
      "MAE_test: 0.3055207091484588\n",
      "Willmott's d Test: 0.9975604716605693\n",
      "Nash-Sutcliffe Test: 0.9899617093285623\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 35.2127 seconds\n",
      "\n",
      "TARGET: MB4\n",
      "\u001b[32m[I 2025-03-25 22:26:14,480]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  1.78it/s]\n",
      "1th fold: XGBRegressor RMSE: 4.3063\n",
      "2th fold: XGBRegressor RMSE: 2.1026\n",
      "3th fold: XGBRegressor RMSE: 4.0515\n",
      "\n",
      "XGBRegressor average RMSE: 3.4868\n",
      "\u001b[32m[I 2025-03-25 22:26:16,166]\u001b[0m Trial 0 finished with value: 3.4868270229849188 and parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}. Best is trial 0 with value: 3.4868270229849188.\u001b[0m\n",
      "3it [00:07,  2.42s/it]\n",
      "1th fold: XGBRegressor RMSE: 3.7438\n",
      "2th fold: XGBRegressor RMSE: 1.8337\n",
      "3th fold: XGBRegressor RMSE: 4.0898\n",
      "\n",
      "XGBRegressor average RMSE: 3.2224\n",
      "\u001b[32m[I 2025-03-25 22:26:23,429]\u001b[0m Trial 1 finished with value: 3.2224216419889964 and parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}. Best is trial 1 with value: 3.2224216419889964.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB4\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.3353611708706813\n",
      "RMSE_test: 1.7190609745486456\n",
      "MAE_test: 1.1268460645733618\n",
      "Willmott's d Test: 0.9501913435873182\n",
      "Nash-Sutcliffe Test: 0.850276216424199\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 48.5030 seconds\n",
      "\n",
      "TARGET: MB8\n",
      "\u001b[32m[I 2025-03-25 22:26:27,769]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  1.72it/s]\n",
      "1th fold: XGBRegressor RMSE: 3.9997\n",
      "2th fold: XGBRegressor RMSE: 1.3812\n",
      "3th fold: XGBRegressor RMSE: 5.9122\n",
      "\n",
      "XGBRegressor average RMSE: 3.7644\n",
      "\u001b[32m[I 2025-03-25 22:26:29,516]\u001b[0m Trial 0 finished with value: 3.764381911672782 and parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}. Best is trial 0 with value: 3.764381911672782.\u001b[0m\n",
      "3it [00:08,  2.74s/it]\n",
      "1th fold: XGBRegressor RMSE: 3.9446\n",
      "2th fold: XGBRegressor RMSE: 0.9846\n",
      "3th fold: XGBRegressor RMSE: 6.0112\n",
      "\n",
      "XGBRegressor average RMSE: 3.6468\n",
      "\u001b[32m[I 2025-03-25 22:26:37,727]\u001b[0m Trial 1 finished with value: 3.6468041155803648 and parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}. Best is trial 1 with value: 3.6468041155803648.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB8\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.9299546711157436\n",
      "RMSE_test: 2.230919279669436\n",
      "MAE_test: 1.5667243807502225\n",
      "Willmott's d Test: 0.4025258884657573\n",
      "Nash-Sutcliffe Test: -2.1998799362867696\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 62.8251 seconds\n",
      "\n",
      "TARGET: MB10\n",
      "\u001b[32m[I 2025-03-25 22:26:42,093]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  1.68it/s]\n",
      "1th fold: XGBRegressor RMSE: 5.8906\n",
      "2th fold: XGBRegressor RMSE: 4.8291\n",
      "3th fold: XGBRegressor RMSE: 3.4479\n",
      "\n",
      "XGBRegressor average RMSE: 4.7225\n",
      "\u001b[32m[I 2025-03-25 22:26:43,879]\u001b[0m Trial 0 finished with value: 4.722539229943547 and parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}. Best is trial 0 with value: 4.722539229943547.\u001b[0m\n",
      "3it [00:08,  2.75s/it]\n",
      "1th fold: XGBRegressor RMSE: 5.6506\n",
      "2th fold: XGBRegressor RMSE: 5.8259\n",
      "3th fold: XGBRegressor RMSE: 3.7469\n",
      "\n",
      "XGBRegressor average RMSE: 5.0745\n",
      "\u001b[32m[I 2025-03-25 22:26:52,127]\u001b[0m Trial 1 finished with value: 5.0744912958795245 and parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}. Best is trial 0 with value: 4.722539229943547.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB10\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.3743303522185292\n",
      "RMSE_test: 3.3393332458192475\n",
      "MAE_test: 2.4351458903338683\n",
      "Willmott's d Test: 0.22365758217273168\n",
      "Nash-Sutcliffe Test: -5.587769998205199\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 74.3272 seconds\n",
      "\n",
      "TARGET: MB18\n",
      "\u001b[32m[I 2025-03-25 22:26:53,596]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:01,  1.71it/s]\n",
      "1th fold: XGBRegressor RMSE: 2.2768\n",
      "2th fold: XGBRegressor RMSE: 1.7777\n",
      "3th fold: XGBRegressor RMSE: 5.3337\n",
      "\n",
      "XGBRegressor average RMSE: 3.1294\n",
      "\u001b[32m[I 2025-03-25 22:26:55,355]\u001b[0m Trial 0 finished with value: 3.1293941143959656 and parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}. Best is trial 0 with value: 3.1293941143959656.\u001b[0m\n",
      "3it [00:07,  2.59s/it]\n",
      "1th fold: XGBRegressor RMSE: 2.6355\n",
      "2th fold: XGBRegressor RMSE: 1.7934\n",
      "3th fold: XGBRegressor RMSE: 5.3805\n",
      "\n",
      "XGBRegressor average RMSE: 3.2698\n",
      "\u001b[32m[I 2025-03-25 22:27:03,141]\u001b[0m Trial 1 finished with value: 3.269802691512884 and parameters: {'n_estimators': 2270, 'learning_rate': 0.002068243584637287, 'max_depth': 10, 'max_leaves': 26, 'colsample_bytree': 0.6061695553391381, 'subsample': 0.5909124836035503, 'reg_alpha': 0.9170225492671691, 'reg_lambda': 1.5212112147976886, 'gamma': 2.6237821581611893}. Best is trial 0 with value: 3.1293941143959656.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB18\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'n_estimators': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'max_leaves': 19, 'colsample_bytree': 0.5780093202212182, 'subsample': 0.5779972601681014, 'reg_alpha': 0.2904180608409973, 'reg_lambda': 4.330880728874676, 'gamma': 3.005575058716044}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.3985588870048597\n",
      "RMSE_test: 3.9909101823922177\n",
      "MAE_test: 3.314719540226395\n",
      "Willmott's d Test: 0.8320130069336114\n",
      "Nash-Sutcliffe Test: 0.010040676951250216\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 85.6071 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./models/XGBoost_procedural.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2a6be5-ae50-4a85-8422-5b81386cd953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET: GV1\n",
      "\u001b[32m[I 2025-03-25 22:27:06,010]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:06,  2.25s/it]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 0.5300\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 0.3176\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 0.8531\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 0.5669\n",
      "\u001b[32m[I 2025-03-25 22:27:12,765]\u001b[0m Trial 0 finished with value: 0.5668980828599784 and parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}. Best is trial 0 with value: 0.5668980828599784.\u001b[0m\n",
      "3it [00:02,  1.01it/s]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 0.5048\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 0.4301\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 0.7720\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 0.5690\n",
      "\u001b[32m[I 2025-03-25 22:27:15,744]\u001b[0m Trial 1 finished with value: 0.5689596818483341 and parameters: {'max_iter': 2003, 'learning_rate': 0.0708101770538266, 'max_depth': 1, 'min_samples_leaf': 30, 'l2_regularization': 4.162213204002109, 'max_features': 0.6061695553391381, 'early_stopping': 'auto'}. Best is trial 0 with value: 0.5668980828599784.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV1\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.01801265926684765\n",
      "RMSE_test: 0.25920722309173394\n",
      "MAE_test: 0.1919866707859037\n",
      "Willmott's d Test: 0.9984350969545124\n",
      "Nash-Sutcliffe Test: 0.9936532004035282\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 12.9264 seconds\n",
      "\n",
      "TARGET: GV3\n",
      "\u001b[32m[I 2025-03-25 22:27:18,889]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:08,  2.80s/it]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 1.0776\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 0.3288\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 1.4734\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 0.9599\n",
      "\u001b[32m[I 2025-03-25 22:27:27,305]\u001b[0m Trial 0 finished with value: 0.959944996450315 and parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}. Best is trial 0 with value: 0.959944996450315.\u001b[0m\n",
      "3it [00:02,  1.01it/s]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 1.1933\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 0.4822\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 1.7254\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 1.1336\n",
      "\u001b[32m[I 2025-03-25 22:27:30,279]\u001b[0m Trial 1 finished with value: 1.1336192127848186 and parameters: {'max_iter': 2003, 'learning_rate': 0.0708101770538266, 'max_depth': 1, 'min_samples_leaf': 30, 'l2_regularization': 4.162213204002109, 'max_features': 0.6061695553391381, 'early_stopping': 'auto'}. Best is trial 0 with value: 0.959944996450315.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV3\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.022532753607108304\n",
      "RMSE_test: 0.735714761028024\n",
      "MAE_test: 0.4914116188642516\n",
      "Willmott's d Test: 0.9915052610228561\n",
      "Nash-Sutcliffe Test: 0.9673043229806079\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 27.8846 seconds\n",
      "\n",
      "TARGET: GV51\n",
      "\u001b[32m[I 2025-03-25 22:27:33,846]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:10,  3.42s/it]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 1.4822\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 0.6103\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 1.1720\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 1.0882\n",
      "\u001b[32m[I 2025-03-25 22:27:44,094]\u001b[0m Trial 0 finished with value: 1.0881671815062457 and parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}. Best is trial 0 with value: 1.0881671815062457.\u001b[0m\n",
      "3it [00:02,  1.00it/s]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 2.3931\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 0.4016\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 0.8178\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 1.2041\n",
      "\u001b[32m[I 2025-03-25 22:27:47,084]\u001b[0m Trial 1 finished with value: 1.2041274385252267 and parameters: {'max_iter': 2003, 'learning_rate': 0.0708101770538266, 'max_depth': 1, 'min_samples_leaf': 30, 'l2_regularization': 4.162213204002109, 'max_features': 0.6061695553391381, 'early_stopping': 'auto'}. Best is trial 0 with value: 1.0881671815062457.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV51\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.023502269207125257\n",
      "RMSE_test: 0.41107721957355037\n",
      "MAE_test: 0.3004031067608176\n",
      "Willmott's d Test: 0.9977044290770427\n",
      "Nash-Sutcliffe Test: 0.9905543644786043\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 46.1178 seconds\n",
      "\n",
      "TARGET: MB4\n",
      "\u001b[32m[I 2025-03-25 22:27:52,075]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:05,  1.79s/it]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 4.9963\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 1.7654\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 3.9782\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 3.5800\n",
      "\u001b[32m[I 2025-03-25 22:27:57,453]\u001b[0m Trial 0 finished with value: 3.5799580264852886 and parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}. Best is trial 0 with value: 3.5799580264852886.\u001b[0m\n",
      "3it [00:02,  1.00it/s]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 4.5251\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 2.6028\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 4.1322\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 3.7534\n",
      "\u001b[32m[I 2025-03-25 22:28:00,444]\u001b[0m Trial 1 finished with value: 3.753352473820082 and parameters: {'max_iter': 2003, 'learning_rate': 0.0708101770538266, 'max_depth': 1, 'min_samples_leaf': 30, 'l2_regularization': 4.162213204002109, 'max_features': 0.6061695553391381, 'early_stopping': 'auto'}. Best is trial 0 with value: 3.5799580264852886.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB4\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.25934948564384586\n",
      "RMSE_test: 1.588718812462887\n",
      "MAE_test: 0.9444776534867054\n",
      "Willmott's d Test: 0.9616986289423384\n",
      "Nash-Sutcliffe Test: 0.8721200943375372\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 55.6758 seconds\n",
      "\n",
      "TARGET: MB8\n",
      "\u001b[32m[I 2025-03-25 22:28:01,638]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:05,  1.92s/it]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 4.2243\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 1.3121\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 5.9946\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 3.8437\n",
      "\u001b[32m[I 2025-03-25 22:28:07,388]\u001b[0m Trial 0 finished with value: 3.843663482457734 and parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}. Best is trial 0 with value: 3.843663482457734.\u001b[0m\n",
      "3it [00:02,  1.01it/s]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 4.0604\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 7.2905\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 6.7597\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 6.0369\n",
      "\u001b[32m[I 2025-03-25 22:28:10,373]\u001b[0m Trial 1 finished with value: 6.0368837523350765 and parameters: {'max_iter': 2003, 'learning_rate': 0.0708101770538266, 'max_depth': 1, 'min_samples_leaf': 30, 'l2_regularization': 4.162213204002109, 'max_features': 0.6061695553391381, 'early_stopping': 'auto'}. Best is trial 0 with value: 3.843663482457734.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB8\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.8604494301923576\n",
      "RMSE_test: 2.1804406293810588\n",
      "MAE_test: 1.548139659633784\n",
      "Willmott's d Test: 0.41723839704097865\n",
      "Nash-Sutcliffe Test: -2.0567118578531227\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 65.3904 seconds\n",
      "\n",
      "TARGET: MB10\n",
      "\u001b[32m[I 2025-03-25 22:28:11,351]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:08,  2.73s/it]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 5.8570\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 5.1271\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 4.0438\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 5.0093\n",
      "\u001b[32m[I 2025-03-25 22:28:19,534]\u001b[0m Trial 0 finished with value: 5.009312666420293 and parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}. Best is trial 0 with value: 5.009312666420293.\u001b[0m\n",
      "3it [00:02,  1.00it/s]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 6.3421\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 15.5336\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 5.8642\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 9.2466\n",
      "\u001b[32m[I 2025-03-25 22:28:22,530]\u001b[0m Trial 1 finished with value: 9.24662379720762 and parameters: {'max_iter': 2003, 'learning_rate': 0.0708101770538266, 'max_depth': 1, 'min_samples_leaf': 30, 'l2_regularization': 4.162213204002109, 'max_features': 0.6061695553391381, 'early_stopping': 'auto'}. Best is trial 0 with value: 5.009312666420293.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB10\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.16195167665618562\n",
      "RMSE_test: 4.25849742718986\n",
      "MAE_test: 2.885934524282623\n",
      "Willmott's d Test: 0.20561185703633866\n",
      "Nash-Sutcliffe Test: -9.713507647012843\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 82.0687 seconds\n",
      "\n",
      "TARGET: MB18\n",
      "\u001b[32m[I 2025-03-25 22:28:28,036]\u001b[0m A new study created in memory with name: hyperparameters_tuning\u001b[0m\n",
      "3it [00:04,  1.61s/it]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 3.1848\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 1.7508\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 5.4631\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 3.4663\n",
      "\u001b[32m[I 2025-03-25 22:28:32,854]\u001b[0m Trial 0 finished with value: 3.466258298604391 and parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}. Best is trial 0 with value: 3.466258298604391.\u001b[0m\n",
      "3it [00:02,  1.01it/s]\n",
      "1th fold: HistGradientBoostingRegressor RMSE: 3.0810\n",
      "2th fold: HistGradientBoostingRegressor RMSE: 2.4845\n",
      "3th fold: HistGradientBoostingRegressor RMSE: 5.1900\n",
      "\n",
      "HistGradientBoostingRegressor average RMSE: 3.5852\n",
      "\u001b[32m[I 2025-03-25 22:28:35,838]\u001b[0m Trial 1 finished with value: 3.5851851173249627 and parameters: {'max_iter': 2003, 'learning_rate': 0.0708101770538266, 'max_depth': 1, 'min_samples_leaf': 30, 'l2_regularization': 4.162213204002109, 'max_features': 0.6061695553391381, 'early_stopping': 'auto'}. Best is trial 0 with value: 3.466258298604391.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB18\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_iter': 1436, 'learning_rate': 0.09507192349792752, 'max_depth': 8, 'min_samples_leaf': 19, 'l2_regularization': 0.7800932022121826, 'max_features': 0.5779972601681014, 'early_stopping': 'auto'}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.8027324823585961\n",
      "RMSE_test: 3.8534946231426064\n",
      "MAE_test: 2.6132709150252404\n",
      "Willmott's d Test: 0.8195117323250621\n",
      "Nash-Sutcliffe Test: 0.07703983560241812\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "Elapsed time: 91.0511 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./models/GBRT_procedural.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
