{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb60609e-9b1f-4738-aacf-42bcdf8f28ea",
   "metadata": {},
   "source": [
    "## Notebook execution in Jupyter for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b04d890-fbb2-47ef-82a5-272960b9bd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/Master-thesis-GBRT-dam'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cdcc61-9e01-4102-9e94-fc1e2c57aa42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2025-03-27 08:16:12,549]\u001b[0m A new study created in memory with name: hyperparameters_tuning_GV1\u001b[0m\n",
      "3it [00:00,  3.24it/s]\n",
      "1th fold: LGBMRegressor RMSE: 61.9984\n",
      "2th fold: LGBMRegressor RMSE: 31.0557\n",
      "3th fold: LGBMRegressor RMSE: 3.2389\n",
      "\n",
      "LGBMRegressor average RMSE: 32.0977\n",
      "LGBMRegressor worst RMSE: 61.9984\n",
      "Corresponding penalty value: 44.0580\n",
      "\u001b[32m[I 2025-03-27 08:16:13,477]\u001b[0m Trial 0 finished with value: 44.0579504912254 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 44.0579504912254.\u001b[0m\n",
      "3it [00:00,  4.64it/s]\n",
      "1th fold: LGBMRegressor RMSE: 31.8711\n",
      "2th fold: LGBMRegressor RMSE: 37.9226\n",
      "3th fold: LGBMRegressor RMSE: 3.5038\n",
      "\n",
      "LGBMRegressor average RMSE: 24.4325\n",
      "LGBMRegressor worst RMSE: 37.9226\n",
      "Corresponding penalty value: 29.8285\n",
      "\u001b[32m[I 2025-03-27 08:16:14,126]\u001b[0m Trial 1 finished with value: 29.82852726778684 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 1 with value: 29.82852726778684.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "3it [00:00,  4.44it/s]\n",
      "1th fold: LGBMRegressor RMSE: 67.7543\n",
      "2th fold: LGBMRegressor RMSE: 1.6221\n",
      "3th fold: LGBMRegressor RMSE: 0.9242\n",
      "\n",
      "LGBMRegressor average RMSE: 23.4335\n",
      "LGBMRegressor worst RMSE: 67.7543\n",
      "Corresponding penalty value: 41.1619\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV1\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.086\n",
      "RMSE_crossval: 23.434\n",
      "RMSE_test: 3.956\n",
      "MAE_test: 2.330\n",
      "Willmott's d Test: 0.794\n",
      "Nash-Sutcliffe Test: -0.478\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "penalty_factor: 0.4\n",
      "Trial time: 3.1884 seconds\n",
      "\n",
      "\u001b[32m[I 2025-03-27 08:16:15,710]\u001b[0m A new study created in memory with name: hyperparameters_tuning_GV3\u001b[0m\n",
      "3it [00:00,  3.37it/s]\n",
      "1th fold: LGBMRegressor RMSE: 5.8662\n",
      "2th fold: LGBMRegressor RMSE: 27.0676\n",
      "3th fold: LGBMRegressor RMSE: 4.4642\n",
      "\n",
      "LGBMRegressor average RMSE: 12.4660\n",
      "LGBMRegressor worst RMSE: 27.0676\n",
      "Corresponding penalty value: 18.3067\n",
      "\u001b[32m[I 2025-03-27 08:16:16,602]\u001b[0m Trial 0 finished with value: 18.306653590093397 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 18.306653590093397.\u001b[0m\n",
      "3it [00:00,  4.15it/s]\n",
      "1th fold: LGBMRegressor RMSE: 12.5801\n",
      "2th fold: LGBMRegressor RMSE: 144.6816\n",
      "3th fold: LGBMRegressor RMSE: 5.8837\n",
      "\n",
      "LGBMRegressor average RMSE: 54.3818\n",
      "LGBMRegressor worst RMSE: 144.6816\n",
      "Corresponding penalty value: 90.5017\n",
      "\u001b[32m[I 2025-03-27 08:16:17,328]\u001b[0m Trial 1 finished with value: 90.50174973298658 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 18.306653590093397.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "3it [00:00,  3.41it/s]\n",
      "1th fold: LGBMRegressor RMSE: 23.9980\n",
      "2th fold: LGBMRegressor RMSE: 2.3719\n",
      "3th fold: LGBMRegressor RMSE: 3.8200\n",
      "\n",
      "LGBMRegressor average RMSE: 10.0633\n",
      "LGBMRegressor worst RMSE: 23.9980\n",
      "Corresponding penalty value: 15.6372\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV3\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.080\n",
      "RMSE_crossval: 10.063\n",
      "RMSE_test: 7.546\n",
      "MAE_test: 3.809\n",
      "Willmott's d Test: 0.609\n",
      "Nash-Sutcliffe Test: -2.439\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "penalty_factor: 0.4\n",
      "Trial time: 3.4868 seconds\n",
      "\n",
      "\u001b[32m[I 2025-03-27 08:16:19,186]\u001b[0m A new study created in memory with name: hyperparameters_tuning_GV51\u001b[0m\n",
      "3it [00:00,  3.33it/s]\n",
      "1th fold: LGBMRegressor RMSE: 16.3293\n",
      "2th fold: LGBMRegressor RMSE: 29.8227\n",
      "3th fold: LGBMRegressor RMSE: 48.1472\n",
      "\n",
      "LGBMRegressor average RMSE: 31.4331\n",
      "LGBMRegressor worst RMSE: 48.1472\n",
      "Corresponding penalty value: 38.1187\n",
      "\u001b[32m[I 2025-03-27 08:16:20,089]\u001b[0m Trial 0 finished with value: 38.118713769107956 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 38.118713769107956.\u001b[0m\n",
      "3it [00:00,  4.13it/s]\n",
      "1th fold: LGBMRegressor RMSE: 128.0073\n",
      "2th fold: LGBMRegressor RMSE: 31.9165\n",
      "3th fold: LGBMRegressor RMSE: 12.7499\n",
      "\n",
      "LGBMRegressor average RMSE: 57.5579\n",
      "LGBMRegressor worst RMSE: 128.0073\n",
      "Corresponding penalty value: 85.7376\n",
      "\u001b[32m[I 2025-03-27 08:16:20,818]\u001b[0m Trial 1 finished with value: 85.73764530648975 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 38.118713769107956.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "3it [00:00,  3.36it/s]\n",
      "1th fold: LGBMRegressor RMSE: 7.0890\n",
      "2th fold: LGBMRegressor RMSE: 8.5304\n",
      "3th fold: LGBMRegressor RMSE: 7.9791\n",
      "\n",
      "LGBMRegressor average RMSE: 7.8662\n",
      "LGBMRegressor worst RMSE: 8.5304\n",
      "Corresponding penalty value: 8.1319\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "GV51\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.098\n",
      "RMSE_crossval: 7.866\n",
      "RMSE_test: 3.021\n",
      "MAE_test: 1.341\n",
      "Willmott's d Test: 0.903\n",
      "Nash-Sutcliffe Test: 0.490\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "penalty_factor: 0.4\n",
      "Trial time: 3.5109 seconds\n",
      "\n",
      "\u001b[32m[I 2025-03-27 08:16:22,698]\u001b[0m A new study created in memory with name: hyperparameters_tuning_MB4\u001b[0m\n",
      "3it [00:00,  3.44it/s]\n",
      "1th fold: LGBMRegressor RMSE: 4.6463\n",
      "2th fold: LGBMRegressor RMSE: 413.1048\n",
      "3th fold: LGBMRegressor RMSE: 916.8139\n",
      "\n",
      "LGBMRegressor average RMSE: 444.8550\n",
      "LGBMRegressor worst RMSE: 916.8139\n",
      "Corresponding penalty value: 633.6386\n",
      "\u001b[32m[I 2025-03-27 08:16:23,573]\u001b[0m Trial 0 finished with value: 633.6385637415729 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 633.6385637415729.\u001b[0m\n",
      "3it [00:00,  4.08it/s]\n",
      "1th fold: LGBMRegressor RMSE: 3.1738\n",
      "2th fold: LGBMRegressor RMSE: 233.1178\n",
      "3th fold: LGBMRegressor RMSE: 1912.9975\n",
      "\n",
      "LGBMRegressor average RMSE: 716.4297\n",
      "LGBMRegressor worst RMSE: 1912.9975\n",
      "Corresponding penalty value: 1195.0568\n",
      "\u001b[32m[I 2025-03-27 08:16:24,310]\u001b[0m Trial 1 finished with value: 1195.0568208653985 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 633.6385637415729.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "3it [00:00,  3.38it/s]\n",
      "1th fold: LGBMRegressor RMSE: 3.9283\n",
      "2th fold: LGBMRegressor RMSE: 314.6201\n",
      "3th fold: LGBMRegressor RMSE: 20.2813\n",
      "\n",
      "LGBMRegressor average RMSE: 112.9433\n",
      "LGBMRegressor worst RMSE: 314.6201\n",
      "Corresponding penalty value: 193.6140\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB4\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.279\n",
      "RMSE_crossval: 112.943\n",
      "RMSE_test: 128.055\n",
      "MAE_test: 52.881\n",
      "Willmott's d Test: 0.031\n",
      "Nash-Sutcliffe Test: -829.813\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "penalty_factor: 0.4\n",
      "Trial time: 3.4992 seconds\n",
      "\n",
      "\u001b[32m[I 2025-03-27 08:16:26,263]\u001b[0m A new study created in memory with name: hyperparameters_tuning_MB8\u001b[0m\n",
      "3it [00:00,  3.14it/s]\n",
      "1th fold: LGBMRegressor RMSE: 253.6638\n",
      "2th fold: LGBMRegressor RMSE: 444.7392\n",
      "3th fold: LGBMRegressor RMSE: 2865.6043\n",
      "\n",
      "LGBMRegressor average RMSE: 1188.0024\n",
      "LGBMRegressor worst RMSE: 2865.6043\n",
      "Corresponding penalty value: 1859.0432\n",
      "\u001b[32m[I 2025-03-27 08:16:27,221]\u001b[0m Trial 0 finished with value: 1859.04316819786 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 1859.04316819786.\u001b[0m\n",
      "3it [00:00,  3.56it/s]\n",
      "1th fold: LGBMRegressor RMSE: 2075.7886\n",
      "2th fold: LGBMRegressor RMSE: 79.3281\n",
      "3th fold: LGBMRegressor RMSE: 365.9835\n",
      "\n",
      "LGBMRegressor average RMSE: 840.3667\n",
      "LGBMRegressor worst RMSE: 2075.7886\n",
      "Corresponding penalty value: 1334.5355\n",
      "\u001b[32m[I 2025-03-27 08:16:28,067]\u001b[0m Trial 1 finished with value: 1334.5354745731543 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 1 with value: 1334.5354745731543.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "3it [00:00,  3.30it/s]\n",
      "1th fold: LGBMRegressor RMSE: 129254.4896\n",
      "2th fold: LGBMRegressor RMSE: 182.3963\n",
      "3th fold: LGBMRegressor RMSE: 78.3998\n",
      "\n",
      "LGBMRegressor average RMSE: 43171.7619\n",
      "LGBMRegressor worst RMSE: 129254.4896\n",
      "Corresponding penalty value: 77604.8530\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB8\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.777\n",
      "RMSE_crossval: 43171.762\n",
      "RMSE_test: 4060.260\n",
      "MAE_test: 1416.330\n",
      "Willmott's d Test: 0.000\n",
      "Nash-Sutcliffe Test: -10599212.820\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "penalty_factor: 0.4\n",
      "Trial time: 3.8539 seconds\n",
      "\n",
      "\u001b[32m[I 2025-03-27 08:16:30,054]\u001b[0m A new study created in memory with name: hyperparameters_tuning_MB10\u001b[0m\n",
      "3it [00:00,  3.03it/s]\n",
      "1th fold: LGBMRegressor RMSE: 8168.4524\n",
      "2th fold: LGBMRegressor RMSE: 3575.4952\n",
      "3th fold: LGBMRegressor RMSE: 98.3806\n",
      "\n",
      "LGBMRegressor average RMSE: 3947.4427\n",
      "LGBMRegressor worst RMSE: 8168.4524\n",
      "Corresponding penalty value: 5635.8466\n",
      "\u001b[32m[I 2025-03-27 08:16:31,046]\u001b[0m Trial 0 finished with value: 5635.846623204429 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 5635.846623204429.\u001b[0m\n",
      "3it [00:01,  2.04it/s]\n",
      "1th fold: LGBMRegressor RMSE: 37633610352.9831\n",
      "2th fold: LGBMRegressor RMSE: 884.3003\n",
      "3th fold: LGBMRegressor RMSE: 186.8118\n",
      "\n",
      "LGBMRegressor average RMSE: 12544537141.3651\n",
      "LGBMRegressor worst RMSE: 37633610352.9831\n",
      "Corresponding penalty value: 22580166426.0123\n",
      "\u001b[32m[I 2025-03-27 08:16:32,519]\u001b[0m Trial 1 finished with value: 22580166426.012276 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 0 with value: 5635.846623204429.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "3it [00:00,  3.01it/s]\n",
      "1th fold: LGBMRegressor RMSE: 3671.7629\n",
      "2th fold: LGBMRegressor RMSE: 3246.1793\n",
      "3th fold: LGBMRegressor RMSE: 163.1936\n",
      "\n",
      "LGBMRegressor average RMSE: 2360.3786\n",
      "LGBMRegressor worst RMSE: 3671.7629\n",
      "Corresponding penalty value: 2884.9323\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB10\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.333\n",
      "RMSE_crossval: 2360.379\n",
      "RMSE_test: 455.509\n",
      "MAE_test: 279.121\n",
      "Willmott's d Test: 0.003\n",
      "Nash-Sutcliffe Test: -122576.940\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "penalty_factor: 0.4\n",
      "Trial time: 4.5340 seconds\n",
      "\n",
      "\u001b[32m[I 2025-03-27 08:16:34,576]\u001b[0m A new study created in memory with name: hyperparameters_tuning_MB18\u001b[0m\n",
      "3it [00:00,  3.28it/s]\n",
      "1th fold: LGBMRegressor RMSE: 207.1185\n",
      "2th fold: LGBMRegressor RMSE: 659.8927\n",
      "3th fold: LGBMRegressor RMSE: 457.2923\n",
      "\n",
      "LGBMRegressor average RMSE: 441.4345\n",
      "LGBMRegressor worst RMSE: 659.8927\n",
      "Corresponding penalty value: 528.8178\n",
      "\u001b[32m[I 2025-03-27 08:16:35,494]\u001b[0m Trial 0 finished with value: 528.8177748426899 and parameters: {'max_depth': 6, 'num_leaves': 29, 'learning_rate': 0.0732020742417224, 'n_estimators': 1997, 'subsample': 0.5780093202212182, 'feature_fraction': 0.32479561626896214, 'min_gain_to_split': 0.8712541825229919, 'reg_alpha': 4.330880728874676, 'reg_lambda': 3.005575058716044, 'linear_tree': True}. Best is trial 0 with value: 528.8177748426899.\u001b[0m\n",
      "3it [00:00,  4.00it/s]\n",
      "1th fold: LGBMRegressor RMSE: 66.2994\n",
      "2th fold: LGBMRegressor RMSE: 144.8678\n",
      "3th fold: LGBMRegressor RMSE: 446.2241\n",
      "\n",
      "LGBMRegressor average RMSE: 219.1304\n",
      "LGBMRegressor worst RMSE: 446.2241\n",
      "Corresponding penalty value: 309.9679\n",
      "\u001b[32m[I 2025-03-27 08:16:36,247]\u001b[0m Trial 1 finished with value: 309.9678881938309 and parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}. Best is trial 1 with value: 309.9678881938309.\u001b[0m\n",
      "Best params from optuna: \n",
      " {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "3it [00:00,  4.08it/s]\n",
      "1th fold: LGBMRegressor RMSE: 81.1872\n",
      "2th fold: LGBMRegressor RMSE: 1203.6651\n",
      "3th fold: LGBMRegressor RMSE: 455.3343\n",
      "\n",
      "LGBMRegressor average RMSE: 580.0622\n",
      "LGBMRegressor worst RMSE: 1203.6651\n",
      "Corresponding penalty value: 829.5034\n",
      "\n",
      "~~~ TARGET ~~~\n",
      "MB18\n",
      "\n",
      "~~~ MODEL ~~~\n",
      "Best parameters: {'max_depth': 12, 'num_leaves': 26, 'learning_rate': 0.021241787676720834, 'n_estimators': 954, 'subsample': 0.5917022549267169, 'feature_fraction': 0.4433937943676302, 'min_gain_to_split': 7.871346474483568, 'reg_alpha': 2.1597250932105787, 'reg_lambda': 1.4561457009902097, 'linear_tree': True}\n",
      "\n",
      "~~~ TEST METRICS ~~~\n",
      "RMSE_train: 0.707\n",
      "RMSE_crossval: 580.062\n",
      "RMSE_test: 1511.181\n",
      "MAE_test: 882.896\n",
      "Willmott's d Test: 0.001\n",
      "Nash-Sutcliffe Test: -141939.475\n",
      "\n",
      "~~~ OTHER STATS ~~~\n",
      "Train data length: 38 months\n",
      "penalty_factor: 0.4\n",
      "Trial time: 3.4384 seconds\n",
      "\n",
      "Total elapsed time: 25.5130 seconds\n",
      "\n",
      "Output saved to LightGBM_output_test_26_3_pen_04.txt\n"
     ]
    }
   ],
   "source": [
    "!python ./models/LightGBM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c953144f-b001-4106-b9eb-e293f4096a3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ./models/XGBoost_procedural.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f2a6be5-ae50-4a85-8422-5b81386cd953",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python ./models/GBRT_procedural.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc86dc5-cf9a-43e3-add4-2197358097b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Tuning finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
